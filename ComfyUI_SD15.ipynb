{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eAiPPq_FJoNa",
        "outputId": "10457ad0-836a-4957-8c68-1cf4c2fdcd83"
      },
      "outputs": [],
      "source": [
        "# @title 1.a Setup environment\n",
        "!pip install -q --upgrade --quiet torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "# Clone the OFFICIAL ComfyUI repository for maximum stability\n",
        "!git clone https://github.com/comfyanonymous/ComfyUI.git /content/ComfyUI\n",
        "!pip install -q -r /content/ComfyUI/requirements.txt\n",
        "\n",
        "# GGUF custom nodes from calcuis\n",
        "!git clone https://github.com/calcuis/gguf.git /content/ComfyUI/custom_nodes/gguf\n",
        "#no requirements.txt\n",
        "\n",
        "print(\"------------------------\")\n",
        "print(\"✅ Environment Setup Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYydwXsPDB6r",
        "outputId": "bdf085c1-41f7-4ea0-beeb-562e92d21c7b"
      },
      "outputs": [],
      "source": [
        "# @title 1.b Select Model\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "class Architecture(Enum):\n",
        "    CHECKPOINT = \"checkpoint\"\n",
        "    MODULAR = \"modular\"\n",
        "\n",
        "def download_model(model_entry, component_type, dest_comfyui_dir):\n",
        "\n",
        "    component = model_entry.get(component_type)\n",
        "\n",
        "    if component.get(\"custom_url\", False) == True:\n",
        "      download_url =component.get('url')\n",
        "    else:\n",
        "      download_url = component.get('url') + component.get('name')\n",
        "\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{download_url}\" -d \"/content/ComfyUI/models/{dest_comfyui_dir}\" -o \"{component.get('name')}\"\n",
        "\n",
        "selected_model = \"base\"\n",
        "\n",
        "models = {\n",
        "  \"AbsReal181\":{\n",
        "    \"architecture\": Architecture.CHECKPOINT,\n",
        "    \"base_size_px\": 512,\n",
        "    \"type\": \"SD15\",\n",
        "    \"checkpoint\":{\n",
        "      \"name\": \"AbsoluteReality_1.8.1_pruned.safetensors\",\n",
        "      \"url\": \"https://huggingface.co/Lykon/AbsoluteReality/resolve/main/\"\n",
        "      }\n",
        "    },\n",
        "  \"Delib6\":{\n",
        "    \"architecture\": Architecture.CHECKPOINT,\n",
        "    \"base_size_px\": 512,\n",
        "    \"type\": \"SD15\",\n",
        "    \"checkpoint\":{\n",
        "      \"name\": \"Deliberate_v6.safetensors\",\n",
        "      \"url\": \"https://huggingface.co/XpucT/Deliberate/resolve/main/\"\n",
        "      }\n",
        "    },\n",
        "  \"DreamS8\":{\n",
        "    \"architecture\": Architecture.CHECKPOINT,\n",
        "    \"base_size_px\": 512,\n",
        "    \"type\": \"SD15\",\n",
        "    \"checkpoint\":{\n",
        "      \"name\": \"DreamShaper_8_pruned.safetensors\",\n",
        "      \"url\": \"https://huggingface.co/Lykon/DreamShaper/resolve/main/\"\n",
        "      }\n",
        "    },\n",
        "  \"epicR_NS\":{\n",
        "    \"architecture\": Architecture.CHECKPOINT,\n",
        "    \"base_size_px\": 512,\n",
        "    \"type\": \"SD15\",\n",
        "    \"checkpoint\":{\n",
        "      \"name\": \"epicrealism_naturalSin.safetensors\",\n",
        "      \"url\": \"https://huggingface.co/TheImposterImposters/epiCRealism-NaturalSin/resolve/main/\"\n",
        "      }\n",
        "    },\n",
        "  \"RV60B1\":{\n",
        "    \"architecture\": Architecture.CHECKPOINT,\n",
        "    \"base_size_px\": 512,\n",
        "    \"use_custom_vae\": True,\n",
        "    \"type\": \"SD15\",\n",
        "    \"checkpoint\":{\n",
        "      \"name\": \"Realistic_Vision_V6.0_NV_B1_fp16.safetensors\",\n",
        "      \"url\": \"https://huggingface.co/SG161222/Realistic_Vision_V6.0_B1_noVAE/resolve/main/\"\n",
        "      },\n",
        "    \"vae\": {\n",
        "      \"name\": \"vae-ft-mse-840000-ema-pruned.safetensors\",\n",
        "      \"url\": \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/\"\n",
        "      },\n",
        "    },\n",
        "  \"ICBINP\":{\n",
        "    \"architecture\": Architecture.CHECKPOINT,\n",
        "    \"base_size_px\": 512,\n",
        "    \"type\": \"SD15\",\n",
        "    \"checkpoint\":{\n",
        "      \"custom_url\": True,\n",
        "      \"name\": \"icbinpICantBelieveIts_mid2024.safetensors\",\n",
        "      \"url\": \"https://civitai.com/api/download/models/667760?type=Model&format=SafeTensor&size=pruned&fp=fp16\"\n",
        "      }\n",
        "    },\n",
        "  \"HUNYDIT\":{\n",
        "    \"architecture\": Architecture.CHECKPOINT,\n",
        "    \"base_size_px\": 1024,\n",
        "    \"type\": \"HUNYUAN_DIT\",\n",
        "    \"checkpoint\":{\n",
        "      \"custom_url\": False,\n",
        "      \"name\": \"hunyuan_dit_1.2.safetensors\",\n",
        "      \"url\": \"https://huggingface.co/comfyanonymous/hunyuan_dit_comfyui/resolve/main/\"\n",
        "      }\n",
        "    },\n",
        "  \"LUMINA2\":{\n",
        "    \"architecture\": Architecture.MODULAR,\n",
        "    \"base_size_px\": 1024,\n",
        "    \"type\": \"LUMINA2\",\n",
        "    \"unet\":{\n",
        "      \"name\": \"lumina2-q8_0.gguf\",\n",
        "      \"url\": \"https://huggingface.co/calcuis/lumina-gguf/resolve/main/\",\n",
        "      \"is_gguf\": True\n",
        "      },\n",
        "    \"vae\": {\n",
        "      \"name\": \"diffusion_pytorch_model.safetensors\",\n",
        "      \"url\": \"https://huggingface.co/Alpha-VLLM/Lumina-Image-2.0/resolve/main/vae/\"\n",
        "      },\n",
        "    \"text_encoder\":{\n",
        "      \"name\": \"gemma_2_2b_fp16.safetensors\",\n",
        "      \"url\": \"https://huggingface.co/calcuis/lumina-gguf/resolve/main/\",\n",
        "      \"type\": \"LUMINA2\",\n",
        "      \"is_gguf\": False\n",
        "      },\n",
        "    },\n",
        "  \"base\":{\n",
        "    \"architecture\": Architecture.CHECKPOINT,\n",
        "    \"base_size_px\": 512,\n",
        "    \"type\": \"SD15\",\n",
        "    \"checkpoint\":{\n",
        "      \"name\": \"v1-5-pruned-emaonly.safetensors\",\n",
        "      \"url\": \"https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/resolve/main/\"\n",
        "      }\n",
        "    },\n",
        "  }\n",
        "\n",
        "model_entry = models.get(selected_model)\n",
        "\n",
        "if model_entry:\n",
        "  if model_entry.get(\"architecture\", \"checkpoint\") == Architecture.CHECKPOINT:\n",
        "\n",
        "    download_model(model_entry, \"checkpoint\", \"checkpoints\")\n",
        "\n",
        "  elif model_entry.get(\"architecture\", \"checkpoint\") == Architecture.MODULAR:\n",
        "\n",
        "    download_model(model_entry, \"unet\", \"unet\")\n",
        "\n",
        "    download_model(model_entry, \"text_encoder\", \"text_encoders\")\n",
        "\n",
        "  else:\n",
        "    raise ValueError(f\"No valid architecture selected.\")\n",
        "\n",
        "  if (model_entry.get(\"use_custom_vae\", False) == True\n",
        "    or model_entry.get(\"architecture\", \"checkpoint\") == Architecture.MODULAR):\n",
        "\n",
        "    download_model(model_entry, \"vae\", \"vae\")\n",
        "\n",
        "\n",
        "  print(\"------------------------\")\n",
        "  print(\"✅ Model Download Complete!\")\n",
        "else:\n",
        "  print(\"------------------------\")\n",
        "  print(\"❌ No valid model selected.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_uIPptNl09O",
        "outputId": "99cbc2d0-8dec-497b-b1cb-148f164b202d"
      },
      "outputs": [],
      "source": [
        "# @title 1.c LoRAs, Embeddings and Postprocessing\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!mkdir -p /content/ComfyUI/models/loras\n",
        "\n",
        "lora_file_root = \"/content/drive/MyDrive/AI/LoRAs/SD1.5\"\n",
        "\n",
        "loras_config = [\n",
        "  {\n",
        "    \"filename\": \"sample_lora_1.safetensors\",\n",
        "    \"strength\": 1.0,\n",
        "    \"type\": \"SD15\",\n",
        "    \"enabled\": False\n",
        "  },\n",
        "  {\n",
        "    \"filename\": \"sample_lora_2.safetensors\",\n",
        "    \"strength\": 0.85,\n",
        "    \"type\": \"SD15\",\n",
        "    \"enabled\": True\n",
        "  },\n",
        "]\n",
        "\n",
        "for lora_config in loras_config:\n",
        "  if (lora_config.get(\"enabled\", False) == True):\n",
        "    lora_filename = lora_config[\"filename\"]\n",
        "    print(f\"Copying {lora_file_root}/{lora_filename}\")\n",
        "    !cp \"{lora_file_root}/{lora_filename}\" \"/content/ComfyUI/models/loras/\"\n",
        "\n",
        "# embeddings\n",
        "if model_entry['type'] == \"SD15\":\n",
        "  src_embeddings_folder = \"/content/drive/MyDrive/AI/Embeddings/SD1.5/\"\n",
        "  with os.scandir(src_embeddings_folder) as entries:\n",
        "    for entry in entries:\n",
        "      if entry.is_file() and entry.name.endswith(\".safetensors\"):\n",
        "        print(f\"Copying {entry.path}\")\n",
        "        !cp \"{entry.path}\" \"/content/ComfyUI/models/embeddings/\"\n",
        "\n",
        "# upscaling\n",
        "\n",
        "upscale_model_name = \"RealESRGAN_x4plus.safetensors\"\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"https://huggingface.co/Comfy-Org/Real-ESRGAN_repackaged/resolve/main/{upscale_model_name}\" -d /content/ComfyUI/models/upscale_models -o {upscale_model_name}\n",
        "\n",
        "print(\"✅ LoRAs, Embeddings and Postprocessing Setup Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPg7XT2LKMJR",
        "outputId": "a48fdd9e-0658-4005-f444-1a979bb66160"
      },
      "outputs": [],
      "source": [
        "# @title 2. Logic\n",
        "\n",
        "# Imports\n",
        "import sys, os, gc, random, pdb\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from IPython.display import display, Image as IPImage\n",
        "\n",
        "sys.path.insert(0, '/content/ComfyUI')\n",
        "\n",
        "from nodes import (\n",
        "    CheckpointLoaderSimple, VAELoader, KSampler, LatentUpscale,\n",
        "    LoraLoader, EmptyLatentImage, VAEDecode, CLIPTextEncode,\n",
        "    UNETLoader, CLIPLoader, SaveImage)\n",
        "from comfy_extras.nodes_upscale_model import (\n",
        "    UpscaleModelLoader, ImageUpscaleWithModel\n",
        ")\n",
        "\n",
        "from custom_nodes.gguf.pig import (\n",
        "    LoaderGGUF, ClipLoaderGGUF, VaeGGUF\n",
        ")\n",
        "\n",
        "def generate_images(params):\n",
        "\n",
        "  model_entry = models.get(selected_model)\n",
        "  if not model_entry:\n",
        "    raise ValueError(f\"Model not found: The key '{selected_model}' does not exist in the models dictionary.\")\n",
        "\n",
        "  with torch.inference_mode():\n",
        "\n",
        "    if (model_entry.get(\"architecture\", \"checkpoint\") == Architecture.CHECKPOINT):\n",
        "\n",
        "      model, text_encoder, vae = CheckpointLoaderSimple().load_checkpoint(\n",
        "                                      model_entry[\"checkpoint\"][\"name\"])\n",
        "\n",
        "    elif (model_entry.get(\"architecture\", \"checkpoint\") == Architecture.MODULAR):\n",
        "\n",
        "      if (model_entry.get(\"unet\").get(\"is_gguf\")):\n",
        "\n",
        "        model = LoaderGGUF().load_model(model_entry.get(\"unet\").get(\"name\"))[0]\n",
        "      else:\n",
        "        #don't forget to add wight_dtype to unet when needed in the dictionary\n",
        "        #possible values \"fp8_e4m3fn\", \"fp8_e4m3fn_fast\", \"fp8_e5m2\"\n",
        "        model = UNETLoader().load_unet(model_entry.get(\"unet\").get(\"name\"),\n",
        "                                       model_entry.get(\"unet\").get(\"weight_dtype\"))[0]\n",
        "\n",
        "      if (model_entry.get(\"text_encoder\").get(\"is_gguf\")):\n",
        "\n",
        "        text_encoder = ClipLoaderGGUF().load_clip(model_entry.get(\"text_encoder\").get(\"name\"),\n",
        "                      type=model_entry.get(\"text_encoder\").get(\"type\", \"stable_diffusion\"))[0]\n",
        "      else:\n",
        "        text_encoder = CLIPLoader().load_clip(model_entry.get(\"text_encoder\").get(\"name\"),\n",
        "                       type=model_entry.get(\"text_encoder\").get(\"type\", \"stable_diffusion\"))[0]\n",
        "    else:\n",
        "      raise ValueError(f\"No valid architecture selected.\")\n",
        "\n",
        "    clip_skip = int(params.get(\"clip_skip\")) if params.get(\"clip_skip\") else 1\n",
        "    if clip_skip and clip_skip > 1:\n",
        "      text_encoder.clip_layer(-clip_skip)\n",
        "\n",
        "    if (model_entry.get(\"use_custom_vae\", False) == True\n",
        "        or (model_entry.get(\"architecture\", \"checkpoint\") == Architecture.MODULAR)):\n",
        "      print(f\"Using custom VAE: {model_entry['vae']['name']}\")\n",
        "      vae = VAELoader().load_vae(model_entry['vae']['name'])[0]\n",
        "\n",
        "    for lora_config in loras_config:\n",
        "      lora_name = lora_config.get(\"filename\")\n",
        "      strength = float(lora_config.get(\"strength\", 1.0))\n",
        "\n",
        "      if (lora_config.get(\"enabled\", False) == True and\n",
        "        strength > 0.0 and\n",
        "        model_entry.get(\"type\", \"SD15\") == lora_config.get(\"type\", \"SD15\")):\n",
        "\n",
        "        patched_model, patched_clip = LoraLoader().load_lora(\n",
        "            model = model,\n",
        "            clip = text_encoder,\n",
        "            lora_name = lora_name,\n",
        "            strength_model = strength,\n",
        "            strength_clip = strength\n",
        "        )\n",
        "\n",
        "        if strength > 0.0:\n",
        "          #load_lora clones model and clip when strenght > 0.0\n",
        "          del model, text_encoder\n",
        "        model = patched_model\n",
        "        text_encoder = patched_clip\n",
        "\n",
        "    positive_cond = CLIPTextEncode().encode(text_encoder, params['prompt'])[0]\n",
        "    negative_cond = CLIPTextEncode().encode(text_encoder, params['neg_prompt'])[0]\n",
        "\n",
        "    base_size = int(model_entry.get('base_size_px', 512))\n",
        "\n",
        "    final_width = int(params.get(\"width\", base_size))\n",
        "    final_height = int(params.get(\"height\", base_size))\n",
        "\n",
        "    final_steps = int(params.get(\"steps\", 25))\n",
        "\n",
        "    final_sampler  = params.get(\"sampler_name\", \"dpmpp_sde\")\n",
        "    final_scheduler = params.get(\"scheduler\", \"karras\")\n",
        "\n",
        "    if params.get(\"hires_fix\", False) == True:\n",
        "\n",
        "      first_width = (base_size if final_width < final_height\n",
        "        else final_width * base_size / final_height)\n",
        "      first_height = (base_size if final_width > final_height\n",
        "        else final_height * base_size / final_width)\n",
        "\n",
        "      first_width = round(first_width/8.0) * 8\n",
        "      first_height = round(first_height/8.0) * 8\n",
        "\n",
        "      first_steps = round(final_steps * 0.4)\n",
        "      final_steps = final_steps - first_steps\n",
        "\n",
        "      first_sampler = \"dpmpp_sde_gpu\"\n",
        "      first_scheduler = \"normal\"\n",
        "\n",
        "    else:\n",
        "      first_width = final_width\n",
        "      first_height = final_height\n",
        "      first_steps = final_steps\n",
        "      first_sampler = final_sampler\n",
        "      first_scheduler = final_scheduler\n",
        "\n",
        "    empty_latent_img = EmptyLatentImage().generate(\n",
        "        first_width, first_height, params['batch_size'])[0]\n",
        "\n",
        "    print(f\"First KSampler pass with empty latent at W:{first_width}xH:{first_height}\")\n",
        "    print(f\"with steps:{first_steps}\")\n",
        "\n",
        "    sampled_latent = KSampler().sample(\n",
        "        model, params['seed'], first_steps, params['cfg'],\n",
        "        first_sampler, first_scheduler,\n",
        "        positive_cond, negative_cond,\n",
        "        empty_latent_img, denoise=1.0)[0]\n",
        "\n",
        "    if params.get(\"hires_fix\", False) == True:\n",
        "\n",
        "      upscaled_latent = LatentUpscale().upscale(sampled_latent, \"nearest-exact\",\n",
        "                                       final_width, final_height, \"disabled\")[0]\n",
        "\n",
        "      print(f\"Second KSampler pass with upscaled latent at W:{final_width}xH:{final_height}\")\n",
        "      print(f\"with steps:{final_steps}\")\n",
        "\n",
        "      final_latent = KSampler().sample(\n",
        "        model, params['seed'], final_steps, params['cfg'],\n",
        "        final_sampler, final_scheduler,\n",
        "        positive_cond, negative_cond,\n",
        "        upscaled_latent, denoise=0.5)[0]\n",
        "\n",
        "      del upscaled_latent\n",
        "\n",
        "    else:\n",
        "      final_latent = sampled_latent\n",
        "\n",
        "\n",
        "    del empty_latent_img\n",
        "    del model, positive_cond, negative_cond\n",
        "\n",
        "    if 'patched_model' in locals():\n",
        "      del patched_model\n",
        "    if 'patched_clip' in locals():\n",
        "      del patched_clip\n",
        "\n",
        "    images = VAEDecode().decode(vae, final_latent);\n",
        "\n",
        "    del vae, sampled_latent, final_latent\n",
        "\n",
        "    if params.get(\"upscale_with_model\", False) == True:\n",
        "      print(f\"Upscaling with model: {upscale_model_name}\")\n",
        "      upscale_model = UpscaleModelLoader().load_model(upscale_model_name)[0]\n",
        "      upscaled_images = ImageUpscaleWithModel().upscale(upscale_model, images[0])\n",
        "      del images, upscale_model\n",
        "      images = upscaled_images\n",
        "\n",
        "    return images\n",
        "\n",
        "def save_and_display_images(tensor_images, generation_params,\n",
        "                            gdrive_output_path = None):\n",
        "\n",
        "  if tensor_images is None or len(tensor_images) == 0:\n",
        "    print(\"No image to save or display.\")\n",
        "    return\n",
        "\n",
        "  with torch.inference_mode():\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S.%f\")[:-3]\n",
        "    prefix = f\"{timestamp}-\"\n",
        "\n",
        "    saved_images = []\n",
        "\n",
        "    for t_image in tensor_images:\n",
        "      cpu_image = t_image.detach().cpu()\n",
        "      s_image = SaveImage().save_images(cpu_image, prefix, prompt=None,\n",
        "                          extra_pnginfo=generation_params)\n",
        "      saved_images.append(s_image['ui']['images'][0])\n",
        "\n",
        "      if gdrive_output_path:\n",
        "        source_path = os.path.join(\"/content/ComfyUI/output\",\n",
        "                                    saved_image['subfolder'],\n",
        "                                    saved_image['filename'])\n",
        "        !cp \"{source_path}\" \"{gdrive_output_path}\"\n",
        "\n",
        "    print(f\"Images saved\")\n",
        "\n",
        "  # Display the image in Colab\n",
        "  for saved_image in saved_images:\n",
        "    print(\"\\n--- Generated Image ---\")\n",
        "    display(IPImage(filename= os.path.join(\"/content/ComfyUI/output\",\n",
        "                                           saved_image['subfolder'],\n",
        "                                           saved_image['filename'])))\n",
        "\n",
        "    print(\"\\n--- Generation Details ---\")\n",
        "    print(f\"Model: {selected_model}\")\n",
        "    for key, value in generation_params.items():\n",
        "        print(f\"{key.replace('_', ' ').capitalize()}: {value}\")\n",
        "    print(\"-------------------------\")\n",
        "\n",
        "def clean_memory():\n",
        "\n",
        "  gc.collect()\n",
        "  if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.ipc_collect()\n",
        "  for obj in list(globals().values()):\n",
        "      if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
        "          del obj\n",
        "  gc.collect()\n",
        "\n",
        "print(\"✅ Logic Setup Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e4S40gfIo8O",
        "outputId": "4a7c75d8-db89-4f01-9770-429b762b4706"
      },
      "outputs": [],
      "source": [
        "# @title Pre-3. Saved Prompts\n",
        "sp_select = 1\n",
        "\n",
        "s_p = \"\"\n",
        "s_n_p = \"\"\n",
        "\n",
        "if sp_select == 1:\n",
        "  s_p = (\"DSLR photography of a black kitten playing with a ball of yarn\")\n",
        "  s_n_p = (\"worst quality, low quality, painting, drawing, sketch, cartoon,\"\n",
        "           \"anime, manga, render, CG, 3d, blurry, deformed, disfigured,\"\n",
        "           \"mutated, bad anatomy, bad art, watermark, signature, label\")\n",
        "\n",
        "elif sp_select == 2:\n",
        "  s_p = (\"(embedding:fcDetailPortrait:1.0), (embedding:AS-YoungV2:1.2), \"\n",
        "    \"realistic professional photographic portrait of a young Woman \"\n",
        "    \"wearing a black Turtleneck, masterpiece, bokeh, DSLR photography\")\n",
        "  s_n_p = (\"(embedding:BadDream:1.0), (embedding:UnrealisticDream:1.2), \"\n",
        "    \"(embedding:FastNegativeV2:1.0)\")\n",
        "\n",
        "print(\"✅ Stored prompt selected!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a2772774bff64dafb9145380fbea1b13",
            "04b5265b8df94fad96a878ec131421fb",
            "d7a8cd1e203b444c87ff9d1a730b1cbe",
            "a6f2b885310541468a09dbac9951f99e",
            "59c3a8c00ea4461d93e68168623a795f",
            "dce3db39bd034b35b5c45d93fada01d6",
            "380b618ff238429fb5682876c847c4f6",
            "23f038d0e8954fce900c5a74f2201191",
            "a277bdea1b9a447696800b6a4dc77851",
            "ca3d31e6198d4cfc80c559453bfcddad",
            "5a9506f859de4c99a3d8dd453e3b8590"
          ]
        },
        "id": "TBW0_wCIKM2K",
        "outputId": "3cf53a04-2d43-4427-a646-780da6975aed"
      },
      "outputs": [],
      "source": [
        "# @title 3. Interface\n",
        "\n",
        "prompt = \"DSLR photography of a black kitten playing with a ball of yarn\" # @param {\"type\":\"string\"}\n",
        "if s_p:\n",
        "  prompt = s_p\n",
        "\n",
        "neg_prompt = \"3D, cartoon, painting, bad quality, low quality\" # @param {\"type\":\"string\"}\n",
        "if s_n_p:\n",
        "  neg_prompt = s_n_p\n",
        "\n",
        "width = 512 # @param {\"type\":\"slider\", \"min\":\"512\", \"max\":\"2048\", \"step\": \"8\"}\n",
        "height = 768 # @param {\"type\":\"slider\", \"min\":\"512\", \"max\":\"2048\", \"step\": \"8\"}\n",
        "steps = 20 # @param {\"type\":\"number\"}\n",
        "cfg = 7.5 # @param {\"type\":\"number\"}\n",
        "sampler_name = \"dpmpp_2m_sde_gpu\" # @param [\"euler\", \"euler_cfg_pp\", \"euler_ancestral\", \"euler_ancestral_cfg_pp\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\", \"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_2s_ancestral_cfg_pp\", \"dpmpp_sde\", \"dpmpp_sde_gpu\", \"dpmpp_2m\", \"dpmpp_2m_cfg_pp\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\", \"ipndm\", \"ipndm_v\", \"deis\", \"res_multistep\", \"res_multistep_cfg_pp\", \"res_multistep_ancestral\", \"res_multistep_ancestral_cfg_pp\", \"gradient_estimation\", \"gradient_estimation_cfg_pp\", \"er_sde\", \"seeds_2\", \"seeds_3\", \"sa_solver\", \"sa_solver_pece\"]\n",
        "scheduler = \"karras\" # @param [\"simple\", \"sgm_uniform\", \"karras\", \"exponential\", \"ddim_uniform\", \"beta\", \"normal\", \"linear_quadratic\", \"kl_optimal\"]\n",
        "hires_fix = False # @param{\"type\":\"boolean\"}\n",
        "upscale_with_model = False # @param{\"type\":\"boolean\"}\n",
        "batch_size = 1 # @param {\"type\":\"number\"}\n",
        "seed = 0 # @param {\"type\":\"number\"}\n",
        "clip_skip = 1 # @param {\"type\" : \"number\" }\n",
        "save_to_gdrive = False # @param{\"type\":\"boolean\"}\n",
        "gdrive_path = \"/content/drive/MyDrive/AI/Images/SD1.5/\" # @param{\"type\":\"string\"}\n",
        "\n",
        "seed = seed if seed else random.randint(0, 2**32 - 1)\n",
        "print(f\"Seed : {seed}\")\n",
        "\n",
        "generation_params = {\n",
        "    \"prompt\": prompt,\n",
        "    \"neg_prompt\": neg_prompt,\n",
        "    \"width\": width,\n",
        "    \"height\": height,\n",
        "    \"steps\": steps,\n",
        "    \"cfg\": cfg,\n",
        "    \"sampler_name\": sampler_name,\n",
        "    \"scheduler\": scheduler,\n",
        "    \"hires_fix\" : hires_fix,\n",
        "    \"upscale_with_model\" : upscale_with_model,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"seed\": seed,\n",
        "    \"clip_skip\": clip_skip\n",
        "}\n",
        "\n",
        "#import warnings\n",
        "#warnings.filterwarnings('ignore')\n",
        "\n",
        "if save_to_gdrive:\n",
        "  drive.mount('/content/drive')\n",
        "  !mkdir -p \"{gdrive_path}\"\n",
        "\n",
        "images = generate_images(generation_params)\n",
        "save_and_display_images(images, generation_params,\n",
        "                        gdrive_path if save_to_gdrive else None)\n",
        "\n",
        "del images\n",
        "clean_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UlDwR3dYgXLa"
      },
      "outputs": [],
      "source": [
        "# @title Clean Memory\n",
        "clean_memory()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04b5265b8df94fad96a878ec131421fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dce3db39bd034b35b5c45d93fada01d6",
            "placeholder": "​",
            "style": "IPY_MODEL_380b618ff238429fb5682876c847c4f6",
            "value": "100%"
          }
        },
        "23f038d0e8954fce900c5a74f2201191": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "380b618ff238429fb5682876c847c4f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59c3a8c00ea4461d93e68168623a795f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a9506f859de4c99a3d8dd453e3b8590": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2772774bff64dafb9145380fbea1b13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04b5265b8df94fad96a878ec131421fb",
              "IPY_MODEL_d7a8cd1e203b444c87ff9d1a730b1cbe",
              "IPY_MODEL_a6f2b885310541468a09dbac9951f99e"
            ],
            "layout": "IPY_MODEL_59c3a8c00ea4461d93e68168623a795f"
          }
        },
        "a277bdea1b9a447696800b6a4dc77851": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6f2b885310541468a09dbac9951f99e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca3d31e6198d4cfc80c559453bfcddad",
            "placeholder": "​",
            "style": "IPY_MODEL_5a9506f859de4c99a3d8dd453e3b8590",
            "value": " 20/20 [00:04&lt;00:00,  4.07it/s]"
          }
        },
        "ca3d31e6198d4cfc80c559453bfcddad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7a8cd1e203b444c87ff9d1a730b1cbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23f038d0e8954fce900c5a74f2201191",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a277bdea1b9a447696800b6a4dc77851",
            "value": 20
          }
        },
        "dce3db39bd034b35b5c45d93fada01d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
