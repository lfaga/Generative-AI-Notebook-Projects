{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "eAiPPq_FJoNa"
   },
   "outputs": [],
   "source": [
    "# @title 1.a Setup environment\n",
    "!pip install -q --upgrade --quiet torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!apt -y install -qq aria2\n",
    "\n",
    "# Clone the OFFICIAL ComfyUI repository for maximum stability\n",
    "!git clone https://github.com/comfyanonymous/ComfyUI.git /content/ComfyUI\n",
    "!pip install -q -r /content/ComfyUI/requirements.txt\n",
    "\n",
    "# GGUF custom nodes from city96\n",
    "!git clone https://github.com/city96/ComfyUI-GGUF.git /content/ComfyUI/custom_nodes/ComfyUI_GGUF\n",
    "!pip install -q -r /content/ComfyUI/custom_nodes/ComfyUI_GGUF/requirements.txt\n",
    "\n",
    "print(\"------------------------\")\n",
    "print(\"✅ Environment Setup Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "VYydwXsPDB6r"
   },
   "outputs": [],
   "source": [
    "# @title 1.b Select Model\n",
    "selected_model = \"base\" # -@param[\"base\", \"Chroma48\"]\n",
    "\n",
    "models = {\n",
    "  \"Chroma48\":{\n",
    "    \"unet\":{\n",
    "      \"name\": \"chroma-unlocked-v48-Q5_K_M.gguf\",\n",
    "      \"url\": \"https://huggingface.co/duyntnet/Chroma-GGUF-All-Versions/resolve/main/chroma-unlocked-v48/\",\n",
    "      \"is_gguf\": True\n",
    "    },\n",
    "    \"vae\": {\n",
    "      \"name\": \"ae.safetensors\",\n",
    "      \"url\": \"https://huggingface.co/lodestones/Chroma/resolve/main/\",\n",
    "      \"is_gguf\": False\n",
    "      },\n",
    "    \"text_encoder\": {\n",
    "      \"name\": \"t5xxl-encoder-fp32-q6_k.gguf\",\n",
    "      \"url\": \"https://huggingface.co/chatpig/t5-v1_1-xxl-encoder-fp32-gguf/resolve/main/\",\n",
    "      \"is_gguf\": True\n",
    "      },\n",
    "    \"type\": \"chroma\"\n",
    "    },\n",
    "  \"base\":{\n",
    "    \"unet\":{\n",
    "      \"name\": \"flux1-schnell-Q4_K_M.gguf\",\n",
    "      \"url\": \"https://huggingface.co/unsloth/FLUX.1-schnell-GGUF/resolve/main/\",\n",
    "      \"is_gguf\": True\n",
    "    },\n",
    "    \"vae\": {\n",
    "      \"name\": \"diffusion_pytorch_model.safetensors\",\n",
    "      \"url\": \"https://huggingface.co/tripathiarpan20/FLUX.1-schnell/resolve/main/vae/\",\n",
    "      \"is_gguf\": False\n",
    "      },\n",
    "    \"text_encoder\": {\n",
    "      \"name\": \"t5xxl-encoder-fp32-q6_k.gguf\",\n",
    "      \"url\": \"https://huggingface.co/chatpig/t5-v1_1-xxl-encoder-fp32-gguf/resolve/main/\",\n",
    "      \"is_gguf\": True\n",
    "      },\n",
    "    \"type\": \"chroma\"\n",
    "    }\n",
    "  }\n",
    "\n",
    "model_entry = models.get(selected_model)\n",
    "if model_entry:\n",
    "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{model_entry['unet']['url'] + model_entry['unet']['name']}\" -d /content/ComfyUI/models/unet -o \"{model_entry['unet']['name']}\"\n",
    "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{model_entry['vae']['url'] + model_entry['vae']['name']}\" -d /content/ComfyUI/models/vae -o \"{model_entry['vae']['name']}\"\n",
    "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{model_entry['text_encoder']['url'] + model_entry['text_encoder']['name']}\" -d /content/ComfyUI/models/text_encoders -o \"{model_entry['text_encoder']['name']}\"\n",
    "  print(\"------------------------\")\n",
    "  print(\"✅ Model Download Complete!\")\n",
    "else:\n",
    "  print(\"------------------------\")\n",
    "  print(\"No valid model selected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_uIPptNl09O"
   },
   "outputs": [],
   "source": [
    "# @title 1.c LoRAs and Postprocessing\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "!mkdir -p /content/ComfyUI/models/loras\n",
    "\n",
    "lora_file_root = \"/content/drive/MyDrive/AI/LoRAs/Flux.1-Dev\"\n",
    "\n",
    "loras_config = [\n",
    "  {\n",
    "    \"filename\": \"sample_lora.safetensors\",\n",
    "    \"strength\": 1.0\n",
    "  }\n",
    "]\n",
    "\n",
    "for lora_config in loras_config:\n",
    "  lora_filename = lora_config[\"filename\"]\n",
    "  !cp \"{lora_file_root}/{lora_filename}\" \"/content/ComfyUI/models/loras/\"\n",
    "\n",
    "# upscaling\n",
    "\n",
    "upscale_model_name = \"RealESRGAN_x4plus.safetensors\"\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"https://huggingface.co/Comfy-Org/Real-ESRGAN_repackaged/resolve/main/{upscale_model_name}\" -d /content/ComfyUI/models/upscale_models -o {upscale_model_name}\n",
    "\n",
    "print(\"✅ LoRAs, Embeddings and Postprocessing Setup Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPg7XT2LKMJR"
   },
   "outputs": [],
   "source": [
    "# @title 2. Logic\n",
    "\n",
    "# Imports\n",
    "import sys, os, gc, random, pdb\n",
    "import torch\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "sys.path.insert(0, '/content/ComfyUI')\n",
    "\n",
    "from nodes import (\n",
    "    UNETLoader, VAELoader, CLIPLoader, LatentUpscale,\n",
    "    LoraLoaderModelOnly, VAEDecode, KSampler,\n",
    "    CLIPTextEncode, SaveImage)\n",
    "\n",
    "from comfy_extras.nodes_upscale_model import (\n",
    "    UpscaleModelLoader, ImageUpscaleWithModel\n",
    ")\n",
    "\n",
    "from comfy_extras.nodes_custom_sampler import (\n",
    "    CFGGuider, SamplerCustomAdvanced,\n",
    "    RandomNoise, KSamplerSelect, BasicScheduler\n",
    ")\n",
    "\n",
    "from comfy_extras.nodes_cond import (\n",
    "    T5TokenizerOptions\n",
    ")\n",
    "\n",
    "from comfy_extras.nodes_sd3 import (\n",
    "    EmptySD3LatentImage\n",
    ")\n",
    "\n",
    "from custom_nodes.ComfyUI_GGUF.nodes import (\n",
    "    UnetLoaderGGUF, CLIPLoaderGGUF\n",
    ")\n",
    "\n",
    "def generate_images(params):\n",
    "\n",
    "  model_entry = models.get(selected_model)\n",
    "  if not model_entry:\n",
    "    raise ValueError(f\"Model not found: The key '{selected_model}' does not exist in the models dictionary.\")\n",
    "\n",
    "  with torch.inference_mode():\n",
    "\n",
    "    if model_entry[\"text_encoder\"][\"is_gguf\"]:\n",
    "      text_encoder = CLIPLoaderGGUF().load_clip(model_entry[\"text_encoder\"][\"name\"],\n",
    "                                                              model_entry[\"type\"])[0]\n",
    "    else:\n",
    "      text_encoder = CLIPLoader().load_clip(model_entry[\"text_encoder\"][\"name\"],\n",
    "                                  type=model_entry[\"type\"], device=\"default\")[0]\n",
    "\n",
    "    prep_clip = T5TokenizerOptions().set_options(text_encoder, 1, 0)[0]\n",
    "    #VRAM warning, set_options() clones the text_encoder\n",
    "\n",
    "    positive_cond = CLIPTextEncode().encode(prep_clip, params['prompt'])[0]\n",
    "    negative_cond = CLIPTextEncode().encode(prep_clip, params['neg_prompt'])[0]\n",
    "\n",
    "    print(\"After loading clip and patching conds\")\n",
    "    print(f\"VRAM Used: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "    #need to load model for guider, 2 clips + model = VRAM disaster\n",
    "    del prep_clip, text_encoder\n",
    "\n",
    "    if model_entry[\"unet\"][\"is_gguf\"]:\n",
    "      model = UnetLoaderGGUF().load_unet(model_entry[\"unet\"][\"name\"])[0]\n",
    "    else:\n",
    "      #need to add weight_dtype to models dictionary\n",
    "      model = UNETLoader().load_unet(model_entry[\"unet\"][\"name\"],\n",
    "                                                 \"fp8_e4m3fn\")[0]\n",
    "\n",
    "    for lora_config in loras_config:\n",
    "      lora_name = lora_config.get(\"filename\")\n",
    "      strength = float(lora_config.get(\"strength\", 1.0))\n",
    "\n",
    "      patched_model = LoraLoaderModelOnly().load_lora_model_only(\n",
    "          model = model,\n",
    "          lora_name = lora_name,\n",
    "          strength_model = strength)[0]\n",
    "      if strength > 0.0:\n",
    "        #load_lora clones model when strenght > 0.0\n",
    "        del model\n",
    "      model = patched_model\n",
    "\n",
    "    print(\"After loading unet and loras and patching unet\")\n",
    "    print(f\"VRAM Used: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "    guider = CFGGuider().get_guider(model, positive_cond, negative_cond,\n",
    "                                                       params['cfg'])[0]\n",
    "\n",
    "    final_width = int(params.get(\"width\", 1024))\n",
    "    final_height = int(params.get(\"height\", 1024))\n",
    "\n",
    "    final_steps = int(params.get(\"steps\", 25))\n",
    "\n",
    "    final_sampler  = params.get(\"sampler_name\", \"euler\")\n",
    "    final_scheduler = params.get(\"scheduler\", \"beta\")\n",
    "\n",
    "    if params.get(\"hires_fix\", False):\n",
    "      base_size = 1024\n",
    "\n",
    "      first_width = (base_size if final_width < final_height\n",
    "        else final_width * base_size / final_height)\n",
    "      first_height = (base_size if final_width > final_height\n",
    "        else final_height * base_size / final_width)\n",
    "\n",
    "      first_width = round(first_width/8.0) * 8\n",
    "      first_height = round(first_height/8.0) * 8\n",
    "\n",
    "      first_steps = round(final_steps * 0.4)\n",
    "      final_steps = final_steps - first_steps\n",
    "\n",
    "      first_sampler = \"euler\"\n",
    "      first_scheduler = \"simple\"\n",
    "\n",
    "    else:\n",
    "      first_width = final_width\n",
    "      first_height = final_height\n",
    "      first_steps = final_steps\n",
    "      first_sampler = final_sampler\n",
    "      first_scheduler = final_scheduler\n",
    "\n",
    "    empty_latent_img = EmptySD3LatentImage().generate(\n",
    "        first_width, first_height, params['batch_size'])[0]\n",
    "\n",
    "    noise = RandomNoise().get_noise(params['seed'])[0]\n",
    "    sampler = KSamplerSelect().get_sampler(first_sampler)[0]\n",
    "    scheduler = BasicScheduler().get_sigmas(model, first_scheduler, first_steps, denoise=1.0)[0]\n",
    "\n",
    "    print(f\"First Custom Sampler pass with empty latent at W:{first_width}xH:{first_height}\")\n",
    "    print(f\"with steps:{first_steps}\")\n",
    "\n",
    "    sampled_latent = SamplerCustomAdvanced().sample(noise, guider, sampler,\n",
    "                                               scheduler, empty_latent_img)[0]\n",
    "\n",
    "    print(\"After first sampling pass:\")\n",
    "    print(f\"VRAM Used: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "    if params.get(\"hires_fix\", False):\n",
    "\n",
    "      upscaled_latent = LatentUpscale().upscale(sampled_latent, \"nearest-exact\",\n",
    "                                       final_width, final_height, \"disabled\")[0]\n",
    "\n",
    "      print(f\"Second KSampler pass with upscaled latent at W:{final_width}xH:{final_height}\")\n",
    "      print(f\"with steps:{final_steps}\")\n",
    "\n",
    "      final_latent = KSampler().sample(\n",
    "        model, params['seed'], final_steps, params['cfg'],\n",
    "        final_sampler, final_scheduler,\n",
    "        positive_cond, negative_cond,\n",
    "        upscaled_latent, denoise=0.5)[0]\n",
    "\n",
    "      del upscaled_latent\n",
    "\n",
    "    else:\n",
    "      final_latent = sampled_latent\n",
    "\n",
    "\n",
    "    del empty_latent_img\n",
    "    del model, positive_cond, negative_cond\n",
    "\n",
    "    if 'patched_model' in locals():\n",
    "      del patched_model\n",
    "\n",
    "    if model_entry[\"vae\"][\"is_gguf\"]:\n",
    "      raise ValueError(f\"There's no VAELoaderGGUF in this library.\")\n",
    "    else:\n",
    "      vae = VAELoader().load_vae(model_entry[\"vae\"][\"name\"])[0]\n",
    "\n",
    "    images = VAEDecode().decode(vae, final_latent);\n",
    "\n",
    "    print(\"After loading vae and decoding images\")\n",
    "    print(f\"VRAM Used: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "    del vae, sampled_latent, final_latent\n",
    "\n",
    "    if params.get(\"upscale_with_model\", False):\n",
    "      print(f\"Upscaling with model: {upscale_model_name}\")\n",
    "      upscale_model = UpscaleModelLoader().load_model(upscale_model_name)[0]\n",
    "      upscaled_images = ImageUpscaleWithModel().upscale(upscale_model, images[0])\n",
    "      del images, upscale_model\n",
    "      images = upscaled_images\n",
    "\n",
    "    return images\n",
    "\n",
    "def save_and_display_images(tensor_images, generation_params):\n",
    "\n",
    "  if tensor_images is None or len(tensor_images) == 0:\n",
    "    print(\"No image to save or display.\")\n",
    "    return\n",
    "\n",
    "  timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S.%f\")[:-3]\n",
    "  prefix = f\"{timestamp}-\"\n",
    "\n",
    "  saved_images = []\n",
    "\n",
    "  for t_image in tensor_images:\n",
    "    cpu_image = t_image.detach().cpu()\n",
    "    s_image = SaveImage().save_images(cpu_image, prefix, prompt=None,\n",
    "                        extra_pnginfo=generation_params)\n",
    "    saved_images.append(s_image['ui']['images'][0])\n",
    "\n",
    "  print(f\"Images saved\")\n",
    "\n",
    "  # Display the image in Colab\n",
    "  for saved_image in saved_images:\n",
    "    print(\"\\n--- Generated Image ---\")\n",
    "    display(IPImage(filename= os.path.join(\"/content/ComfyUI/output\",\n",
    "                                           saved_image['subfolder'],\n",
    "                                           saved_image['filename'])))\n",
    "\n",
    "    print(\"\\n--- Generation Details ---\")\n",
    "    print(f\"Model: {models[selected_model]['unet']['name']}\")\n",
    "    for key, value in generation_params.items():\n",
    "        print(f\"{key.replace('_', ' ').capitalize()}: {value}\")\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "def clean_memory():\n",
    "  gc.collect()\n",
    "  if torch.cuda.is_available():\n",
    "      torch.cuda.empty_cache()\n",
    "      torch.cuda.ipc_collect()\n",
    "  for obj in list(globals().values()):\n",
    "      if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
    "          del obj\n",
    "  gc.collect()\n",
    "\n",
    "print(\"✅ Logic Setup Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "9e4S40gfIo8O"
   },
   "outputs": [],
   "source": [
    "# @title Pre-3. Saved Prompts\n",
    "sp_select = 1\n",
    "\n",
    "s_p = \"\"\n",
    "s_n_p = \"\"\n",
    "\n",
    "if sp_select == 1:\n",
    "  s_p = (\"DSLR photography of a black kitten playing with a ball of yarn\")\n",
    "  s_n_p = (\"worst quality, low quality, painting, drawing, sketch, cartoon,\"\n",
    "           \"anime, manga, render, CG, 3d, blurry, deformed, disfigured,\"\n",
    "           \"mutated, bad anatomy, bad art, watermark, signature, label\")    \n",
    "\n",
    "print(\"✅ Stored prompt selected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "TBW0_wCIKM2K"
   },
   "outputs": [],
   "source": [
    "# @title 3. Execute\n",
    "\n",
    "prompt = \"DSLR photography of a black kitten playing with a ball of yarn\" # @param {\"type\":\"string\"}\n",
    "if s_p:\n",
    "  prompt = s_p\n",
    "\n",
    "neg_prompt = \"3D, cartoon, painting, bad quality, low quality\" # @param {\"type\":\"string\"}\n",
    "if s_n_p:\n",
    "  neg_prompt = s_n_p\n",
    "\n",
    "width = 768 # @param {\"type\":\"slider\", \"min\":\"512\", \"max\":\"2048\", \"step\": \"16\"}\n",
    "height = 1024 # @param {\"type\":\"slider\", \"min\":\"512\", \"max\":\"2048\", \"step\": \"16\"}\n",
    "steps = 6 # @param {\"type\":\"number\"}\n",
    "cfg = 1.5 # @param {\"type\":\"number\"}\n",
    "sampler_name = \"euler\" # @param [\"euler\", \"euler_cfg_pp\", \"euler_ancestral\", \"euler_ancestral_cfg_pp\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\", \"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_2s_ancestral_cfg_pp\", \"dpmpp_sde\", \"dpmpp_sde_gpu\", \"dpmpp_2m\", \"dpmpp_2m_cfg_pp\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\", \"ipndm\", \"ipndm_v\", \"deis\", \"res_multistep\", \"res_multistep_cfg_pp\", \"res_multistep_ancestral\", \"res_multistep_ancestral_cfg_pp\", \"gradient_estimation\", \"gradient_estimation_cfg_pp\", \"er_sde\", \"seeds_2\", \"seeds_3\", \"sa_solver\", \"sa_solver_pece\"]\n",
    "scheduler = \"beta\" # @param [\"simple\", \"sgm_uniform\", \"karras\", \"exponential\", \"ddim_uniform\", \"beta\", \"normal\", \"linear_quadratic\", \"kl_optimal\"]\n",
    "hires_fix = False # @param{\"type\":\"boolean\"}\n",
    "upscale_with_model = False # @param{\"type\":\"boolean\"}\n",
    "batch_size = 1 # @param {\"type\":\"number\"}\n",
    "seed = 0 # @param {\"type\":\"number\"}\n",
    "clip_skip = 1 # @param {\"type\" : \"number\" }\n",
    "\n",
    "if seed == 0:\n",
    "  seed = random.randint(0, 2**32 - 1)\n",
    "print(f\"Seed : {seed}\")\n",
    "\n",
    "generation_params = {\n",
    "    \"prompt\": prompt,\n",
    "    \"neg_prompt\": neg_prompt,\n",
    "    \"width\": width,\n",
    "    \"height\": height,\n",
    "    \"steps\": steps,\n",
    "    \"cfg\": cfg,\n",
    "    \"sampler_name\": sampler_name,\n",
    "    \"scheduler\": scheduler,\n",
    "    \"hires_fix\" : hires_fix,\n",
    "    \"upscale_with_model\" : upscale_with_model,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"seed\": seed,\n",
    "    \"clip_skip\": clip_skip\n",
    "}\n",
    "\n",
    "images = generate_images(generation_params)\n",
    "save_and_display_images(images, generation_params)\n",
    "\n",
    "del images\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlDwR3dYgXLa"
   },
   "outputs": [],
   "source": [
    "# @title Clean Memory\n",
    "clean_memory()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
