{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2gwCJ8f3mjA"
   },
   "source": [
    "# **WAN Fusion X IMAGE TO VIDEO WITH FAST Q3 GGUF MODEL**\n",
    "- QuantStack/Wan2.1_I2V_14B_FusionX-GGUF\n",
    "- The videos are generated at 16fps. You can use the `Frame Interpolation` notebook in this github repository (https://github.com/Isi-dev/Google-Colab_Notebooks) to increase it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t089iwSddWDL"
   },
   "outputs": [],
   "source": [
    "# @title Prepare Environment\n",
    "!pip install --upgrade --quiet torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "%cd /content\n",
    "\n",
    "!pip install -q torchsde einops diffusers accelerate xformers\n",
    "!pip install av\n",
    "\n",
    "%cd /content\n",
    "\n",
    "# 1. Clone the OFFICIAL ComfyUI repository for maximum stability\n",
    "!git clone https://github.com/comfyanonymous/ComfyUI.git\n",
    "%cd /content/ComfyUI\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# 2. Install the community-recommended GGUF custom node from city96\n",
    "%cd /content/ComfyUI/custom_nodes\n",
    "!git clone https://github.com/city96/ComfyUI-GGUF.git\n",
    "# The cloned directory has a hyphen, but Python needs an underscore for imports.\n",
    "# We rename the directory to make it a valid Python package.\n",
    "!mv /content/ComfyUI/custom_nodes/ComfyUI-GGUF /content/ComfyUI/custom_nodes/ComfyUI_GGUF\n",
    "# Now, continue with the custom node's requirements installation\n",
    "%cd /content/ComfyUI/custom_nodes/ComfyUI_GGUF\n",
    "!pip install -r requirements.txt\n",
    "%cd /content/ComfyUI\n",
    "\n",
    "!apt -y install -qq aria2 ffmpeg\n",
    "\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/QuantStack/Wan2.1_I2V_14B_FusionX-GGUF/resolve/main/Wan2.1_I2V_14B_FusionX-Q3_K_M.gguf -d /content/ComfyUI/models/unet -o Wan2.1_I2V_14B_FusionX-Q3_K_M.gguf\n",
    "\n",
    "encoder_filename = \"umt5-xxl-encoder-Q8_0.gguf\"\n",
    "#!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors -d /content/ComfyUI/models/text_encoders -o umt5_xxl_fp8_e4m3fn_scaled.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/city96/umt5-xxl-encoder-gguf/resolve/main/umt5-xxl-encoder-Q8_0.gguf -d /content/ComfyUI/models/text_encoders -o umt5-xxl-encoder-Q8_0.gguf\n",
    "\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors -d /content/ComfyUI/models/vae -o wan_2.1_vae.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors -d /content/ComfyUI/models/clip_vision -o clip_vision_h.safetensors\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import gc\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import imageio\n",
    "import subprocess\n",
    "from google.colab import files\n",
    "from IPython.display import display, HTML, Image as IPImage\n",
    "from google.colab import drive\n",
    "\n",
    "sys.path.insert(0, '/content/ComfyUI')\n",
    "\n",
    "from comfy import model_management\n",
    "\n",
    "from nodes import (\n",
    "    CheckpointLoaderSimple,\n",
    "    CLIPLoader,\n",
    "    CLIPTextEncode,\n",
    "    VAEDecode,\n",
    "    VAELoader,\n",
    "    KSampler,\n",
    "    UNETLoader,\n",
    "    LoadImage,\n",
    "    CLIPVisionLoader,\n",
    "    CLIPVisionEncode,\n",
    "    LoraLoader\n",
    ")\n",
    "\n",
    "from custom_nodes.ComfyUI_GGUF.nodes import UnetLoaderGGUF, CLIPLoaderGGUF\n",
    "\n",
    "from comfy_extras.nodes_model_advanced import ModelSamplingSD3\n",
    "from comfy_extras.nodes_wan import WanImageToVideo\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "!mkdir -p /content/ComfyUI/models/loras\n",
    "\n",
    "lora_filename = \"sample_lora.safetensors\"\n",
    "lora_strength_value = 1.0\n",
    "!cp \"/content/drive/MyDrive/AI/LoRAs/Wan2.1/{lora_filename}\" \"/content/ComfyUI/models/loras/\"\n",
    "\n",
    "def clear_memory():\n",
    "    # This is the command that tells the ComfyUI backend to release its grip on the models.\n",
    "    from comfy import model_management\n",
    "    model_management.unload_all_models()\n",
    "\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    for obj in list(globals().values()):\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
    "            del obj\n",
    "    gc.collect()\n",
    "\n",
    "def save_as_mp4(images, filename_prefix, fps, output_dir=\"/content/ComfyUI/output\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = f\"{output_dir}/{filename_prefix}.mp4\"\n",
    "\n",
    "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
    "\n",
    "    with imageio.get_writer(output_path, fps=fps) as writer:\n",
    "        for frame in frames:\n",
    "            writer.append_data(frame)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "def save_as_webp(images, filename_prefix, fps, quality=90, lossless=False, method=4, output_dir=\"/content/ComfyUI/output\"):\n",
    "    \"\"\"Save images as animated WEBP using imageio.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = f\"{output_dir}/{filename_prefix}.webp\"\n",
    "\n",
    "\n",
    "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
    "\n",
    "\n",
    "    kwargs = {\n",
    "        'fps': int(fps),\n",
    "        'quality': int(quality),\n",
    "        'lossless': bool(lossless),\n",
    "        'method': int(method)\n",
    "    }\n",
    "\n",
    "    with imageio.get_writer(\n",
    "        output_path,\n",
    "        format='WEBP',\n",
    "        mode='I',\n",
    "        **kwargs\n",
    "    ) as writer:\n",
    "        for frame in frames:\n",
    "            writer.append_data(frame)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "def save_as_webm(images, filename_prefix, fps, codec=\"vp9\", quality=32, output_dir=\"/content/ComfyUI/output\"):\n",
    "    \"\"\"Save images as WEBM using imageio.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = f\"{output_dir}/{filename_prefix}.webm\"\n",
    "\n",
    "\n",
    "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
    "\n",
    "\n",
    "    kwargs = {\n",
    "        'fps': int(fps),\n",
    "        'quality': int(quality),\n",
    "        'codec': str(codec),\n",
    "        'output_params': ['-crf', str(int(quality))]\n",
    "    }\n",
    "\n",
    "    with imageio.get_writer(\n",
    "        output_path,\n",
    "        format='FFMPEG',\n",
    "        mode='I',\n",
    "        **kwargs\n",
    "    ) as writer:\n",
    "        for frame in frames:\n",
    "            writer.append_data(frame)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "def save_as_image(image, filename_prefix, output_dir=\"/content/ComfyUI/output\"):\n",
    "    \"\"\"Save single frame as PNG image.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = f\"{output_dir}/{filename_prefix}.png\"\n",
    "\n",
    "    frame = (image.cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "    Image.fromarray(frame).save(output_path)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def upload_image():\n",
    "    \"\"\"Handle image upload in Colab and store in /content/ComfyUI/input/\"\"\"\n",
    "    from google.colab import files\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    os.makedirs('/content/ComfyUI/input', exist_ok=True)\n",
    "\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    # Move each uploaded file to ComfyUI input directory\n",
    "    for filename in uploaded.keys():\n",
    "        src_path = f'/content/ComfyUI/{filename}'\n",
    "        dest_path = f'/content/ComfyUI/input/{filename}'\n",
    "\n",
    "        shutil.move(src_path, dest_path)\n",
    "        print(f\"Image saved to: {dest_path}\")\n",
    "        return dest_path\n",
    "\n",
    "    return None\n",
    "\n",
    "def generate_video(\n",
    "    image_path: str = None,\n",
    "    positive_prompt: str = \"a cute cat playing with a ball of yarn\",\n",
    "    negative_prompt: str = \"overexposed, blurred details, subtitles, worst quality, low quality, JPEG compression artifacts, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, fused fingers, still picture, three legs, walking backwards, watermark, text, signature\",\n",
    "    width: int = 832,\n",
    "    height: int = 480,\n",
    "    seed: int = 82628696717253,\n",
    "    steps: int = 4,\n",
    "    cfg_scale: float = 1.0,\n",
    "    sampler_name: str = \"uni_pc\",\n",
    "    scheduler: str = \"simple\",\n",
    "    frames: int = 33,\n",
    "    fps: int = 16,\n",
    "    output_format: str = \"mp4\"\n",
    "):\n",
    "    import pdb\n",
    "\n",
    "    # Initialize nodes\n",
    "    unet_loader = UnetLoaderGGUF()\n",
    "    model_sampling = ModelSamplingSD3()\n",
    "    #clip_loader = CLIPLoader()\n",
    "    clip_loader = CLIPLoaderGGUF()\n",
    "    clip_encode_positive = CLIPTextEncode()\n",
    "    clip_encode_negative = CLIPTextEncode()\n",
    "    vae_loader = VAELoader()\n",
    "    clip_vision_loader = CLIPVisionLoader()\n",
    "    clip_vision_encode = CLIPVisionEncode()\n",
    "    load_image = LoadImage()\n",
    "    wan_image_to_video = WanImageToVideo()\n",
    "    ksampler = KSampler()\n",
    "    vae_decode = VAEDecode()\n",
    "    load_lora = LoraLoader()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # --- Stage 1: Patch the CLIP Model ---\n",
    "        print(f\"Loading Text_Encoder: {encoder_filename}\")\n",
    "        clip = clip_loader.load_clip(encoder_filename, \"wan\")[0]\n",
    "\n",
    "        print(\"Loading Unet Model...\")\n",
    "        model = unet_loader.load_unet(\"Wan2.1_I2V_14B_FusionX-Q3_K_M.gguf\")[0]\n",
    "\n",
    "        print(f\"Applying LoRA '{lora_filename}' to CLIP model...\")\n",
    "        # We call lora_loader, returns a tuple: (patched_model, patched_clip).\n",
    "        patched_model, patched_clip = load_lora.load_lora(\n",
    "            model=model,\n",
    "            clip=clip,\n",
    "            lora_name=lora_filename,\n",
    "            strength_model=lora_strength_value,\n",
    "            strength_clip=lora_strength_value\n",
    "        )\n",
    "\n",
    "        # --- Stage 2: Encode prompts using the now-patched CLIP ---\n",
    "        print(\"Encoding prompts with LoRA-aware CLIP...\")\n",
    "        positive = clip_encode_positive.encode(patched_clip, positive_prompt)[0]\n",
    "        negative = clip_encode_negative.encode(patched_clip, negative_prompt)[0]\n",
    "\n",
    "        #pdb.set_trace() #for checking VRAM usage at this point\n",
    "\n",
    "        # --- Stage 3: Purge CLIP from VRAM ---\n",
    "        # and can now delete the original, un-patched UNET model\n",
    "        print(\"Releasing CLIP model from VRAM...\")\n",
    "        del clip\n",
    "        del patched_clip\n",
    "        del model\n",
    "        # Aggressive cleanup: The loaders and original models are no longer needed.\n",
    "        del load_lora, unet_loader, clip_loader\n",
    "        del clip_encode_positive, clip_encode_negative\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        # --- Stage 4: Prepare Image, VAE, and Latents ---\n",
    "        # (This section is the same as before)\n",
    "        if image_path is None: image_path = upload_image()\n",
    "        if image_path is None: return\n",
    "\n",
    "        loaded_image = load_image.load_image(image_path)[0]\n",
    "        clip_vision = clip_vision_loader.load_clip(\"clip_vision_h.safetensors\")[0]\n",
    "        clip_vision_output = clip_vision_encode.encode(clip_vision, loaded_image, \"none\")[0]\n",
    "        del clip_vision\n",
    "        gc.collect()\n",
    "\n",
    "        vae = vae_loader.load_vae(\"wan_2.1_vae.safetensors\")[0]\n",
    "\n",
    "        positive_out, negative_out, latent = wan_image_to_video.encode(\n",
    "            positive, negative, vae, width, height, frames, 1, loaded_image, clip_vision_output\n",
    "        )\n",
    "\n",
    "        # We patch the *already LoRA-patched* model again with ModelSampling\n",
    "        final_model = model_sampling.patch(patched_model, 8)[0]\n",
    "\n",
    "        # Delete everything not needed for the final KSampler step.\n",
    "        del positive, negative, loaded_image, clip_vision_output\n",
    "        del load_image, clip_vision_loader, clip_vision_encode, vae_loader, wan_image_to_video\n",
    "        del patched_model, model_sampling\n",
    "        gc.collect()\n",
    "\n",
    "        # --- Stage 5: Generate the video ---\n",
    "        #pdb.set_trace() #for checking VRAM usage at this point\n",
    "        print(\"Generating video...\")\n",
    "        sampled = ksampler.sample(\n",
    "            model=final_model, # Use the final patched UNET\n",
    "            seed=seed,\n",
    "            steps=steps,\n",
    "            cfg=cfg_scale,\n",
    "            sampler_name=sampler_name,\n",
    "            scheduler=scheduler,\n",
    "            positive=positive_out,\n",
    "            negative=negative_out,\n",
    "            latent_image=latent\n",
    "        )[0]\n",
    "\n",
    "        del final_model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        # --- Stage 6: Decode and save ---\n",
    "        # (This section is the same as before)\n",
    "        try:\n",
    "            decoded = vae_decode.decode(vae, sampled)[0]\n",
    "            del vae\n",
    "            gc.collect()\n",
    "            # ... rest of saving logic ...\n",
    "            if frames == 1:\n",
    "                output_path = save_as_image(decoded[0], \"ComfyUI\")\n",
    "                display(IPImage(filename=output_path))\n",
    "            else:\n",
    "                if output_format.lower() == \"mp4\":\n",
    "                    output_path = save_as_mp4(decoded, \"ComfyUI\", fps)\n",
    "                else: # Defaulting to webm for any other case\n",
    "                    output_path = save_as_webm(decoded, \"ComfyUI\", fps=fps, codec=\"vp9\", quality=10)\n",
    "                display_video(output_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during decoding/saving: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            # Explicitly delete all node objects\n",
    "            del ksampler, vae_decode\n",
    "            clear_memory()\n",
    "\n",
    "def display_video(video_path):\n",
    "    from IPython.display import HTML\n",
    "    from base64 import b64encode\n",
    "\n",
    "    video_data = open(video_path,'rb').read()\n",
    "\n",
    "    # Determine MIME type based on file extension\n",
    "    if video_path.lower().endswith('.mp4'):\n",
    "        mime_type = \"video/mp4\"\n",
    "    elif video_path.lower().endswith('.webm'):\n",
    "        mime_type = \"video/webm\"\n",
    "    elif video_path.lower().endswith('.webp'):\n",
    "        mime_type = \"image/webp\"\n",
    "    else:\n",
    "        mime_type = \"video/mp4\"  # default\n",
    "\n",
    "    data_url = f\"data:{mime_type};base64,\" + b64encode(video_data).decode()\n",
    "\n",
    "    display(HTML(f\"\"\"\n",
    "    <video width=512 controls autoplay loop>\n",
    "        <source src=\"{data_url}\" type=\"{mime_type}\">\n",
    "    </video>\n",
    "    \"\"\"))\n",
    "\n",
    "print(\"âœ… Environment Setup Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "wo8w6tKerJMJ"
   },
   "outputs": [],
   "source": [
    "# @title Generate Video\n",
    "\n",
    "positive_prompt = \"a black kitten playing with a ball of yarn\" # @param {\"type\":\"string\"}\n",
    "negative_prompt = \"overexposed, blurred details, subtitles, worst quality, low quality, JPEG compression artifacts, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, fused fingers, still picture, three legs, walking backwards, watermark, text, signature\" # @param {\"type\":\"string\"}\n",
    "width = 480 # @param {\"type\":\"number\"}\n",
    "height = 832 # @param {\"type\":\"number\"}\n",
    "seed = 42 # @param {\"type\":\"integer\"}\n",
    "steps = 4 # @param {\"type\":\"integer\", \"min\":1, \"max\":100}\n",
    "cfg_scale = 1.0 # @param {\"type\":\"number\", \"min\":1, \"max\":20}\n",
    "sampler_name = \"uni_pc\" # @param [\"uni_pc\", \"euler\", \"dpmpp_2m\", \"ddim\", \"lms\"]\n",
    "scheduler = \"simple\" # @param [\"simple\", \"normal\", \"karras\", \"exponential\"]\n",
    "frames = 61 # @param {\"type\":\"integer\", \"min\":1, \"max\":121}\n",
    "fps = 16 # @param {\"type\":\"integer\", \"min\":1, \"max\":60}\n",
    "output_format = \"mp4\" # @param [\"mp4\", \"webm\"]\n",
    "\n",
    "import random\n",
    "seed = seed if seed != 0 else random.randint(0, 2**32 - 1)\n",
    "print(f\"Using seed: {seed}\")\n",
    "\n",
    "# with torch.inference_mode():\n",
    "generate_video(\n",
    "    image_path=None,\n",
    "    positive_prompt=positive_prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    width=width,\n",
    "    height=height,\n",
    "    seed=seed,\n",
    "    steps=steps,\n",
    "    cfg_scale=cfg_scale,\n",
    "    sampler_name=sampler_name,\n",
    "    scheduler=scheduler,\n",
    "    frames=frames,\n",
    "    fps=fps,\n",
    "    output_format=output_format\n",
    ")\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "0yKqpf78AlbC"
   },
   "outputs": [],
   "source": [
    "# @title Clear Memory in case of stopping execution\n",
    "clear_memory()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/Isi-dev/Google-Colab_Notebooks/blob/main/Wan2_1_14B_I2V_GGUF_Free.ipynb",
     "timestamp": 1748249329096
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
