{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "193ZMICgB-Dt"
   },
   "outputs": [],
   "source": [
    "#@title 1. Installs & Imports\n",
    "# Install necessary libraries\n",
    "!pip install diffusers transformers accelerate safetensors invisible_watermark --quiet\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, AutoencoderKL\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "from google.colab import drive\n",
    "import random\n",
    "\n",
    "# Suppress a common warning from diffusers\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"diffusers.utils.loading_utils\")\n",
    "\n",
    "print(\"Libraries installed and imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvbZoJFCCTbm"
   },
   "outputs": [],
   "source": [
    "#@title 2. Configuration\n",
    "\n",
    "# --- Model & VAE Configuration ---\n",
    "BASE_MODEL_ID = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "# You can use a specific VAE or set to None to use the default from the base model\n",
    "VAE_MODEL_ID = \"stabilityai/sd-vae-ft-mse\"\n",
    "\n",
    "# --- Output Configuration ---\n",
    "# Path in your Google Drive to save images. Set to None to disable saving to Drive.\n",
    "# Example: \"/content/drive/MyDrive/AI_Images/SD1.5_Generated\"\n",
    "OUTPUT_DIR_GDRIVE = \"/content/drive/MyDrive/AI/Images/SD15\"\n",
    "\n",
    "# --- LoRA Configuration ---\n",
    "# List of LoRAs to load.\n",
    "# Each LoRA is a dictionary with:\n",
    "#   \"source\": \"hf\" (Hugging Face) or \"gdrive\" (Google Drive)\n",
    "#   \"id_or_path\": Hugging Face model ID or full path in Google Drive\n",
    "#                 (e.g., \"MyLoRAs/your_lora.safetensors\" - path relative to \"My Drive\")\n",
    "#   \"adapter_name\": A unique name for this LoRA adapter (used internally by diffusers)\n",
    "#   \"weight\": The weight to apply to this LoRA (0.0 to 1.0 typically)\n",
    "#   \"trigger_words\": A string of trigger words, comma-separated if multiple.\n",
    "#                    These will be appended to your prompt.\n",
    "\n",
    "LORAS_CONFIG = [\n",
    "    {\n",
    "        \"source\": \"hf\",\n",
    "        \"id_or_path\": \"OedoSoldier/detail-tweaker-lora\", # This is the Detail Tweaker LoRA\n",
    "        \"adapter_name\": \"add_detail\",\n",
    "        \"weight\": 0.3, # This LoRA is powerful, start with a lower weight like 0.5\n",
    "        \"trigger_words\": \"add_detail\" # The trigger word for this LoRA\n",
    "    },\n",
    "    #{\n",
    "    #     \"source\": \"gdrive\",\n",
    "    #     \"id_or_path\": \"/content/drive/MyDrive/AI/LoRAs/SD1.5/sample-lora.safetensors\",\n",
    "    #     \"adapter_name\": \"sample_lora\",\n",
    "    #     \"weight\": 0.6,\n",
    "    #     \"trigger_words\": \"sample_lora\"\n",
    "    #},\n",
    "    # Add more LoRAs here if needed\n",
    "]\n",
    "\n",
    "# --- Device Configuration ---\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    torch_dtype = torch.float16 # Use float16 for GPU for faster inference and less memory\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    torch_dtype = torch.float32 # float32 for CPU\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Using dtype: {torch_dtype}\")\n",
    "if LORAS_CONFIG:\n",
    "    print(f\"Configured {len(LORAS_CONFIG)} LoRA(s).\")\n",
    "else:\n",
    "    print(\"No LoRAs configured.\")\n",
    "if OUTPUT_DIR_GDRIVE:\n",
    "    print(f\"Images will be saved to: {OUTPUT_DIR_GDRIVE}\")\n",
    "else:\n",
    "    print(\"Image saving to Google Drive is disabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "O9wEAMhkCTUJ"
   },
   "outputs": [],
   "source": [
    "#@title 3. Helper Functions\n",
    "\n",
    "def mount_google_drive():\n",
    "    \"\"\"Mounts Google Drive to Colab.\"\"\"\n",
    "    try:\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"Google Drive mounted successfully.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error mounting Google Drive: {e}\")\n",
    "        return False\n",
    "\n",
    "def sanitize_filename(text, max_length=60):\n",
    "    \"\"\"Sanitizes text for use in filenames.\"\"\"\n",
    "    if not text:\n",
    "        text = \"untitled\"\n",
    "    # Remove invalid characters\n",
    "    text = re.sub(r'[\\\\/*?:\"<>|]', \"\", text)\n",
    "    # Replace spaces with underscores\n",
    "    text = text.replace(\" \", \"_\")\n",
    "    # Truncate to max_length\n",
    "    return text[:max_length]\n",
    "\n",
    "def load_base_pipeline(base_model_id, vae_model_id, device, dtype):\n",
    "    \"\"\"Loads the base Stable Diffusion pipeline, optionally with a custom VAE.\"\"\"\n",
    "    print(f\"Loading base model: {base_model_id}\")\n",
    "    components = {}\n",
    "    if vae_model_id:\n",
    "        print(f\"Loading custom VAE: {vae_model_id}\")\n",
    "        try:\n",
    "            vae = AutoencoderKL.from_pretrained(vae_model_id, torch_dtype=dtype)\n",
    "            components[\"vae\"] = vae\n",
    "            print(\"Custom VAE loaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load custom VAE {vae_model_id}. Error: {e}. Using default VAE.\")\n",
    "\n",
    "    try:\n",
    "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "            base_model_id,\n",
    "            torch_dtype=dtype,\n",
    "            **components # Unpack VAE if loaded\n",
    "        )\n",
    "        pipeline = pipeline.to(device)\n",
    "        pipeline.enable_attention_slicing() # Recommended for saving memory\n",
    "        print(\"Base pipeline loaded and moved to device.\")\n",
    "        return pipeline\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal Error: Could not load base model {base_model_id}. Error: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_and_prepare_loras(pipeline, loras_config_list, device):\n",
    "    \"\"\"Loads LoRAs from Hugging Face or Google Drive and prepares them for use.\"\"\"\n",
    "    if not loras_config_list:\n",
    "        print(\"No LoRAs to load.\")\n",
    "        return []\n",
    "\n",
    "    loaded_lora_details = []\n",
    "    gdrive_lora_paths_to_check = []\n",
    "\n",
    "    for lora_config in loras_config_list:\n",
    "        source = lora_config.get(\"source\")\n",
    "        id_or_path = lora_config.get(\"id_or_path\")\n",
    "        adapter_name = lora_config.get(\"adapter_name\")\n",
    "        weight = lora_config.get(\"weight\", 0.7) # Default weight if not specified\n",
    "        trigger_words = lora_config.get(\"trigger_words\", \"\")\n",
    "\n",
    "        if not id_or_path or not adapter_name:\n",
    "            print(f\"Skipping LoRA due to missing id_or_path or adapter_name: {lora_config}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Preparing to load LoRA '{adapter_name}' from {source}: {id_or_path}\")\n",
    "\n",
    "        lora_filename_or_id = id_or_path\n",
    "        if source == \"gdrive\":\n",
    "            # Construct full path for GDrive LoRAs\n",
    "            full_gdrive_path = os.path.join(\"/content/drive/MyDrive\", id_or_path)\n",
    "            if not os.path.exists(full_gdrive_path):\n",
    "                print(f\"Warning: LoRA file not found at GDrive path: {full_gdrive_path}. Skipping this LoRA.\")\n",
    "                gdrive_lora_paths_to_check.append(full_gdrive_path) # For a summary warning\n",
    "                continue\n",
    "            lora_filename_or_id = full_gdrive_path\n",
    "            print(f\"  Loading LoRA from GDrive path: {lora_filename_or_id}\")\n",
    "        elif source == \"hf\":\n",
    "            print(f\"  Loading LoRA from Hugging Face ID: {lora_filename_or_id}\")\n",
    "        else:\n",
    "            print(f\"Warning: Unknown LoRA source '{source}' for {adapter_name}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            pipeline.load_lora_weights(\n",
    "                lora_filename_or_id, # HF model_id or local path\n",
    "                adapter_name=adapter_name\n",
    "            )\n",
    "            loaded_lora_details.append({\n",
    "                \"name\": adapter_name,\n",
    "                \"weight\": weight,\n",
    "                \"triggers\": trigger_words\n",
    "            })\n",
    "            print(f\"  Successfully loaded LoRA '{adapter_name}'. Weight: {weight}, Triggers: '{trigger_words}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading LoRA '{adapter_name}' from {lora_filename_or_id}: {e}\")\n",
    "\n",
    "    if gdrive_lora_paths_to_check:\n",
    "        print(\"\\n--- GDrive LoRA File Check ---\")\n",
    "        print(\"The following GDrive LoRA paths were configured but the files were not found:\")\n",
    "        for p_check in gdrive_lora_paths_to_check:\n",
    "            print(f\"  - {p_check}\")\n",
    "        print(\"Please ensure these files exist in your Google Drive at the specified paths.\")\n",
    "        print(\"--------------------------------\\n\")\n",
    "\n",
    "\n",
    "    if loaded_lora_details:\n",
    "        print(f\"\\nSuccessfully loaded {len(loaded_lora_details)} LoRA(s).\")\n",
    "    else:\n",
    "        print(\"\\nNo LoRAs were successfully loaded.\")\n",
    "    return loaded_lora_details\n",
    "\n",
    "\n",
    "def generate_image(pipeline, prompt, negative_prompt, width, height, num_steps, guidance_scale, seed, active_lora_details_list):\n",
    "    \"\"\"Generates an image using the pipeline and applies active LoRAs.\"\"\"\n",
    "    print(\"\\n--- Starting Image Generation ---\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    if negative_prompt:\n",
    "        print(f\"Negative Prompt: {negative_prompt}\")\n",
    "    print(f\"Dimensions: {width}x{height}, Steps: {num_steps}, CFG: {guidance_scale}, Seed: {seed}\")\n",
    "\n",
    "    generator = torch.Generator(device=DEVICE).manual_seed(seed)\n",
    "\n",
    "    active_adapter_names = [lora['name'] for lora in active_lora_details_list]\n",
    "    active_adapter_weights = [lora['weight'] for lora in active_lora_details_list]\n",
    "\n",
    "    if active_adapter_names:\n",
    "        print(f\"Activating LoRAs: {active_adapter_names} with weights: {active_adapter_weights}\")\n",
    "        try:\n",
    "            # Fusing and unfusing can be more memory efficient for multiple LoRAs\n",
    "            # but set_adapters is simpler for typical use cases.\n",
    "            # For many LoRAs, consider fuse/unfuse:\n",
    "            # pipeline.fuse_lora(adapter_names=active_adapter_names, lora_scale=1.0) # lora_scale here is different from individual weights\n",
    "            # For simplicity, we'll use set_adapters which respects individual weights directly.\n",
    "            pipeline.set_adapters(active_adapter_names, adapter_weights=active_adapter_weights)\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting LoRA adapters: {e}. Proceeding without LoRAs.\")\n",
    "            active_adapter_names = [] # Clear if error\n",
    "    else:\n",
    "        print(\"No LoRAs active for this generation.\")\n",
    "\n",
    "\n",
    "    image = None\n",
    "    try:\n",
    "        with torch.autocast(DEVICE if DEVICE == \"cuda\" else \"cpu\"): # Autocast for mixed precision on CUDA\n",
    "            image_result = pipeline(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                width=width,\n",
    "                height=height,\n",
    "                num_inference_steps=num_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                generator=generator,\n",
    "                num_images_per_prompt=1\n",
    "            )\n",
    "        image = image_result.images[0]\n",
    "        print(\"Image generated successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during image generation: {e}\")\n",
    "    finally:\n",
    "        if active_adapter_names:\n",
    "            # print(\"Disabling/resetting LoRAs...\")\n",
    "            # pipeline.unfuse_lora() # if fuse_lora was used\n",
    "            pipeline.disable_lora() # Simpler, or pipeline.set_adapters([], [])\n",
    "            # print(\"LoRAs disabled.\")\n",
    "        pass # No explicit disable needed if set_adapters is used per call and no LoRAs for next\n",
    "\n",
    "    return image\n",
    "\n",
    "def save_and_display_image(image, output_dir, base_filename_prompt, generation_params):\n",
    "    \"\"\"Saves the image with a timestamped filename and displays it.\"\"\"\n",
    "    if not image:\n",
    "        print(\"No image to save or display.\")\n",
    "        return\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    sanitized_prompt = sanitize_filename(base_filename_prompt)\n",
    "    filename = f\"{timestamp}_{sanitized_prompt}.png\"\n",
    "\n",
    "    if output_dir:\n",
    "        if not os.path.exists(output_dir):\n",
    "            try:\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                print(f\"Created output directory: {output_dir}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating directory {output_dir}: {e}. Image will not be saved to Drive.\")\n",
    "                output_dir = None # Prevent save attempt\n",
    "\n",
    "        if output_dir: # Check again in case makedirs failed\n",
    "            save_path = os.path.join(output_dir, filename)\n",
    "            try:\n",
    "                image.save(save_path)\n",
    "                print(f\"Image saved to: {save_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving image to {save_path}: {e}\")\n",
    "\n",
    "    # Display the image in Colab\n",
    "    print(\"\\n--- Generated Image ---\")\n",
    "    display(image) # display() is a Colab-specific function for rich output\n",
    "\n",
    "    print(\"\\n--- Generation Details ---\")\n",
    "    for key, value in generation_params.items():\n",
    "        print(f\"{key.replace('_', ' ').capitalize()}: {value}\")\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "0s_S9YBsCTLr"
   },
   "outputs": [],
   "source": [
    "#@title 4. Mount Drive (if needed) & Load Models\n",
    "# Mount Google Drive if an output directory is specified or GDrive LoRAs are configured\n",
    "needs_drive_mount = bool(OUTPUT_DIR_GDRIVE) or any(lora['source'] == 'gdrive' for lora in LORAS_CONFIG)\n",
    "\n",
    "if needs_drive_mount:\n",
    "    print(\"Google Drive access is required for saving images or loading GDrive LoRAs.\")\n",
    "    if not mount_google_drive():\n",
    "        print(\"Warning: Google Drive not mounted. GDrive LoRAs will not load, and images will not be saved to Drive.\")\n",
    "        # If drive mount fails, disable GDrive specific features that depend on it\n",
    "        if OUTPUT_DIR_GDRIVE:\n",
    "            print(f\"Disabling saving to {OUTPUT_DIR_GDRIVE}.\")\n",
    "            OUTPUT_DIR_GDRIVE = None # Disable saving if mount fails\n",
    "        # GDrive LoRAs will be skipped by load_and_prepare_loras if path doesn't exist\n",
    "else:\n",
    "    print(\"Google Drive mount not required based on current configuration.\")\n",
    "\n",
    "# --- Load Base Pipeline & VAE ---\n",
    "try:\n",
    "    pipeline = load_base_pipeline(BASE_MODEL_ID, VAE_MODEL_ID, DEVICE, torch_dtype)\n",
    "except Exception as e:\n",
    "    print(f\"Stopping execution due to critical error in loading base pipeline: {e}\")\n",
    "    raise # Or handle more gracefully depending on desired behavior\n",
    "\n",
    "# --- Load Configured LoRAs ---\n",
    "if pipeline: # Only proceed if pipeline loaded successfully\n",
    "    loaded_lora_details = load_and_prepare_loras(pipeline, LORAS_CONFIG, DEVICE)\n",
    "else:\n",
    "    loaded_lora_details = []\n",
    "    print(\"Skipping LoRA loading as base pipeline failed to load.\")\n",
    "\n",
    "print(\"\\nModel and LoRA loading process complete.\")\n",
    "if not pipeline:\n",
    "    print(\"CRITICAL: Pipeline was not loaded. Image generation will not be possible.\")\n",
    "elif not loaded_lora_details and LORAS_CONFIG:\n",
    "    print(\"WARNING: Some LoRAs were configured but none were successfully loaded. Check GDrive paths and HF IDs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "uwXN13_PCS4X"
   },
   "outputs": [],
   "source": [
    "#@title 5. Image Generation Parameters (Form)\n",
    "#@markdown ---\n",
    "#@markdown **Prompting:**\n",
    "base_prompt_text = \"DSLR photography of a black kitten playing with a ball of yarn.\" #@param {type:\"string\"}\n",
    "negative_prompt_text = \"bad quality, worse quality, blurry\" #@param {type:\"string\"}\n",
    "#@markdown ---\n",
    "#@markdown **Image Dimensions (SD1.5 best works with 512x512 or 512x768, etc.):**\n",
    "image_width = 512 #@param {type:\"slider\", min:256, max:1024, step:64}\n",
    "image_height = 768 #@param {type:\"slider\", min:256, max:1024, step:64}\n",
    "#@markdown ---\n",
    "#@markdown **Generation Settings:**\n",
    "num_inference_steps = 30 #@param {type:\"slider\", min:10, max:100, step:1}\n",
    "guidance_scale = 6 #@param {type:\"slider\", min:1, max:20, step:0.5}\n",
    "seed_value = -1 #@param {type:\"integer\"}\n",
    "#@markdown *Setting seed to -1 will use a random seed for each generation.*\n",
    "#@markdown ---\n",
    "#@markdown **LoRA Usage:**\n",
    "#@markdown *All successfully loaded LoRAs (from Cell 2 & 4) will be active with their configured weights and trigger words.*\n",
    "#@markdown *If you want to disable a LoRA, comment it out in the `LORAS_CONFIG` in Cell 2 and re-run Cell 2 and Cell 4.*\n",
    "\n",
    "print(\"Parameters form ready. Adjust above and run the next cell to generate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_FSymSYjCoio"
   },
   "outputs": [],
   "source": [
    "#@title 6. Generate Image!\n",
    "\n",
    "if 'pipeline' not in globals() or pipeline is None:\n",
    "    print(\"Pipeline not loaded. Please run Cell 4 (Load Models) first.\")\n",
    "else:\n",
    "    # --- Prepare for Generation ---\n",
    "    # Get parameters from the form (Cell 5)\n",
    "    current_base_prompt = base_prompt_text\n",
    "    current_negative_prompt = negative_prompt_text\n",
    "    current_width = image_width\n",
    "    current_height = image_height\n",
    "    current_steps = num_inference_steps\n",
    "    current_cfg = guidance_scale\n",
    "    current_seed = seed_value\n",
    "\n",
    "    if current_seed == -1:\n",
    "        current_seed = random.randint(0, 2**32 - 1)\n",
    "        print(f\"Using random seed: {current_seed}\")\n",
    "\n",
    "    # Construct the full prompt with LoRA trigger words\n",
    "    full_prompt_parts = [current_base_prompt]\n",
    "    if 'loaded_lora_details' in globals() and loaded_lora_details: # Check if var exists and is not empty\n",
    "        for lora in loaded_lora_details:\n",
    "            if lora.get(\"triggers\"): # Only add if triggers are defined\n",
    "                full_prompt_parts.append(lora[\"triggers\"])\n",
    "    final_prompt = \", \".join(filter(None, full_prompt_parts)) # Join non-empty parts\n",
    "\n",
    "    # --- Call Generation Function ---\n",
    "    generated_image = generate_image(\n",
    "        pipeline,\n",
    "        prompt=final_prompt,\n",
    "        negative_prompt=current_negative_prompt,\n",
    "        width=current_width,\n",
    "        height=current_height,\n",
    "        num_steps=current_steps,\n",
    "        guidance_scale=current_cfg,\n",
    "        seed=current_seed,\n",
    "        active_lora_details_list=loaded_lora_details if 'loaded_lora_details' in globals() else []\n",
    "    )\n",
    "\n",
    "    # --- Save and Display ---\n",
    "    if generated_image:\n",
    "        generation_params_summary = {\n",
    "            \"base_prompt\": current_base_prompt,\n",
    "            \"full_prompt_used\": final_prompt,\n",
    "            \"negative_prompt\": current_negative_prompt,\n",
    "            \"dimensions\": f\"{current_width}x{current_height}\",\n",
    "            \"steps\": current_steps,\n",
    "            \"cfg_scale\": current_cfg,\n",
    "            \"seed\": current_seed,\n",
    "            \"loras_active\": [f\"{lora['name']} (w:{lora['weight']})\" for lora in loaded_lora_details] if 'loaded_lora_details' in globals() and loaded_lora_details else \"None\"\n",
    "        }\n",
    "        save_and_display_image(\n",
    "            generated_image,\n",
    "            OUTPUT_DIR_GDRIVE, # Uses the global variable from Cell 2\n",
    "            current_base_prompt, # For filename\n",
    "            generation_params_summary\n",
    "        )\n",
    "    else:\n",
    "        print(\"Image generation failed. No image to display or save.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPcXn7jwLK4IcJi8juoCT9I",
   "gpuType": "T4",
   "mount_file_id": "1gQTSTyLiizXIuGEf9uKlsG26MIQ3ekb1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
